# GitHub Scraper - AI-Powered Code Extraction & Microservices Generation

## ğŸš€ Project Overview
GitHub Scraper is an AI-driven system that discovers, fetches, and processes public repository code from GitHub. It uses a **generalized GitHub scraper** and an **AI-powered agent** to extract code and transform it into **Pydantic-based input/output specifications and microservices**. The extracted models and services can be dynamically loaded, making this an ideal solution for **automated API generation** and **microservices architecture**.

## âœ¨ Features
- ğŸ” **Automated GitHub Repository Scraping** - Extracts code from public repositories.
- ğŸ§  **AI-Powered Code Parsing** - Converts raw code into structured Pydantic models.
- âš¡ **Dynamic Microservice Generation** - Auto-generates APIs based on extracted code.
- ğŸ”‘ **Secure API Authentication** - Uses API key authentication to restrict access.
- ğŸ“Š **Optimized Caching & Performance** - Implements async optimizations and in-memory caching.
- ğŸ›¡ **Rate Limiting & Security** - Prevents abuse with built-in rate limiting.
- ğŸ“„ **Detailed API Documentation** - Auto-generated Swagger documentation for easy exploration.
- âœ… **Automated Testing Suite** - Ensures reliability with unit and integration tests.

## ğŸ— Project Structure
```
ğŸ“‚ Github Scraper
 â”œâ”€â”€ ğŸ“‚ models               # Auto-generated Pydantic models
 â”œâ”€â”€ ğŸ“‚ microservices        # Dynamically created microservices
 â”œâ”€â”€ ğŸ“‚ tests                # Unit & integration tests
 â”œâ”€â”€ ğŸ“„ main.py              # FastAPI application entry point
 â”œâ”€â”€ ğŸ“„ requirements.txt      # Python dependencies
 â”œâ”€â”€ ğŸ“„ Dockerfile            # Docker containerization setup
 â”œâ”€â”€ ğŸ“„ README.md             # Project documentation
 â”œâ”€â”€ ğŸ“„ .env                  # Environment variables (API keys, etc.)
```

## ğŸ”§ Setup & Installation
### 1ï¸âƒ£ Prerequisites
- **Python 3.11+**
- **pip** (Python package manager)
- **Redis** (For caching, optional but recommended)
- **Docker** (Optional for containerization)

### 2ï¸âƒ£ Clone the Repository
```sh
https://github.com/Keensight-GPT-Applications-2/GitHub-AI-App---Scraper-and-Agent
cd github-scraper
```

### 3ï¸âƒ£ Install Dependencies
```sh
pip install -r requirements.txt
```

### 4ï¸âƒ£ Set Up Environment Variables
Create a **.env** file in the project root with:
```
API_KEY=your_secure_api_key
```

### 5ï¸âƒ£ Run the FastAPI Server
```sh
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

### 6ï¸âƒ£ Access API Documentation
Once the server is running, open your browser and visit:
- **Swagger UI**: [http://localhost:8000/docs](http://localhost:8000/docs)
- **Redoc**: [http://localhost:8000/redoc](http://localhost:8000/redoc)

## ğŸ” API Authentication
All API endpoints are secured using API key authentication.
Include the API key in your request headers:
```
X-API-KEY: your_secure_api_key
```

## ğŸ›  API Endpoints
### âœ… Root Endpoint
```http
GET /
```
**Response:**
```json
{
  "message": "Welcome to the Pydantic Microservice!"
}
```

### âœ… List Available Models
```http
GET /models
```
**Response:**
```json
{
  "models": ["Model1", "Model2", "Model3"]
}
```

### âœ… Fetch a Specific Model Schema
```http
GET /models/{model_name}
```
**Response:**
```json
{
  "title": "Model1",
  "type": "object",
  "properties": { ... }
}
```

## ğŸ§ª Running Tests
Run all test cases using:
```sh
pytest tests/
```

## ğŸš€ Docker Deployment (Optional)
### 1ï¸âƒ£ Build the Docker Image
```sh
docker build -t github_scraper .
```

### 2ï¸âƒ£ Run the Docker Container
```sh
docker run -d -p 8000:8000 --name github_scraper_container github_scraper
```

### 3ï¸âƒ£ Verify the Running Container
```sh
docker ps
```

## ğŸ“Œ Future Improvements
- ğŸ”— **GitHub Webhooks Integration** for real-time repository tracking.
- ğŸ“¡ **Enhanced AI Model** to improve code parsing accuracy.
- ğŸŒ **Multi-Language Support** for extracting and analyzing code in multiple languages.
- â˜ **Cloud Deployment** with Kubernetes for scalable deployments.
